---
title: "Practical Machine Learning Assignment "
output: html_document
---


###Executive summary
Three models were created to predict the quality of the performance of a prescribed exercise based on 52 predictor variables.  The data for these variables was obtained from six movement sensors attached to the body of the person performing the exercise.   After the intial assessment steps, the random forest model was considered the most accurate predictive model.  When the random forest model was applied to predict 20 semi-independent events, the model had an astonishing 100% success rate.  One of the random forest algorithms strengths is it combines numerous 'weak predictors' to form 'strong predictors', which may help explain part of this success.  Overall, the model can be considered an excellent method to predict 'exercise effectiveness' given these 52 dependent variables.  Further work could further identify the most critical of those predictor variables and how far this accuracy extends. 


###Background
Much of the current personal analysis of exercise, including data collated by modern devices such as Jawbone Up, Nike FuelBand, and Fitbit, focusses on the quantity of exercise performed, with less attention paid to the ‘quality’ of the exercise performed.  In this study, the movements of six participants, who had limited previous experience lifting weights, were captured using accelerometers as they performed barbell lifts with light weights, and a score (from A to E) was given for the 'correctness' of the performance of the exercise.  This base data was then analysed, and using machine learning functions three models were created to predict the technical competency of the performance of the exercise given various predictor variables (the sensor data).  The resulting model was then used to predict whether an exercise was correctly performed on an semi-independent testing dataset.  

The dataset is publically available under the Creative Commons license (CC BY-SA) licence at http://groupware.les.inf.puc-rio.br/har

###Exploratory data analysis
First, the original data must be loaded into R
```{r, message = FALSE}
mydata<-read.csv("c:/pml_assignment/ass_train.csv")
```

Then the data table was reviewed by eye to identify any superfluous data columns.  A simple cleaning process removed columns with no data, a significant proportion of NA’s, or probable limited predictive value (eg names).  The rating for the technical skill with which the exercise was performed (the A to E scoring) was also converted to a form more readily usable by R.
```{r}
mydata$classe <- factor(mydata$classe)
mydata<-subset(mydata, select = c(8:11, 37:49, 60:68,84:86, 102, 113:124, 140, 151:160))
```


The initial large dataset must be split into training (from which the models are initially constructed) and testing (the dataset used to test the models accuracy before the final review) subsets.  The standard 60:40 split has been utilised.
```{r, message = FALSE}
library(caret)
set.seed(676767)   # for repeatability
srcData <- createDataPartition(y = mydata$classe, p=0.6,list=FALSE);
train <- mydata[srcData,];
test <- mydata[-srcData,];
```


###Model selection
Then the actual models are created.  In order to try and ensure the highest possible accuracy in the forecast, three models will be built and assessed for accuracy against the test subset of data.  The functions used to create the models will use all 52 available variables.  The most accurate model will then be used to predict the exercise quality in the review data.  The three models will be using the random forest,  boosted trees and linear discriminant algorithms respectively.  
```{r, message = FALSE}
library(randomForest)
rf <- randomForest( classe ~., data=train, ntrees=20)
library(MASS)
lda1<- lda(classe ~. , train)
library(gbm)
gbm_model<- gbm(classe ~. ,data= mydata, distribution = "gaussian" ,n.trees = 30)
```

###Initial review
The predictive power of the three models is first assessed by predicting the exercise effectiveness for the original test dataset used to create them, and checking their accuracy.

For the random forest (rf) model, the test assessment shows: 
```{r}
rf_train <- predict(rf, train)
print(confusionMatrix(rf_train, train$classe))
```
Basically, this shows the random forest model to be 100% accurate, with a p-value of effectively 0.  Generally a result of this nature would raise significant concerns over the model (such as it MUST be overfitting), and would render much statistical analysis unreliable (eg ANOVA).  However, for this exercise the model's results are accepted as feasible, and not an error, subject to later review.

Given the extremely high predictive power of the random forest model, review of the other models appears futile and so will not be undertaken.  Basically, they are extremely unlikely to be as good.

###Cross validation
The  random forecast model chosen will be the best for the test dataset, as the model was chosen for exactly that reason.  To try and identify, minimise, and possibly remove any inherent errors or overfitting, a cross validation process was undertaken.  This will also give an indication as to how the model may evaluate the review (final semi-independent) dataset.
```{r}
rf_test <- predict(rf, test)
print(confusionMatrix(rf_test, test$classe))
```

The cross validation process shows the model remains extremely accurate when predicting against the test dataset, with accuracy of 99.36%,  and p-value remaining at effectively 0. 

The OOB (out of bag) error rate constructed as the initial rf model was built shows roughly an overall OOB error rate of just under 1% (0.99%) for the models trees.  
```{r}
t<- sum(rf$err.rate[1:500,1])/500
t
```

However on the final prediction model, a simple calculation of the total out of sample error rate shows it has shrunk to 0.637%.  In part this could be attributable to the random forest's efficient use of multiple decision trees to minimise overfitting from the individual trees.
```{r}
rf_err <-sum(rf_test != test$classe)
rf_total <- nrow(test)
round(rf_err/rf_total,5)
```

Considering the large number of dependent variables, the low error rate is unusual (eg the accumulated noise over those variables would be expected to have some impact).  

However, based on the analysis it appears reasonable to apply the chosen model to the final semi-independent review dataset.  

###Prediction
This is the final step and can only be undertaken once.  If done more than this, the independent data would effectively become part of the intial dataset as the model will be influenced to fit this data.  

First the independent data file is read into R, then the random forest model is applied, and the resultant predictions shown.
```{r}
rev<-read.csv("c:/pml_assignment/ass_review.csv")
rf_predict <- predict(rf,rev)
rf_predict
```

The results are then transferred to a vector array, a separate text file is created for each predicted value and these were assessed against the actual results.  

```{r, eval=FALSE}
setwd("c:/pml_assignment/")                   # set working dir
answers <- as.vector(rf_review)    # create vector list
pml_write_files(answers)          
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
```

The review showed the model to be 100% accurate, an astonishing hit rate.  It would be interesting to assess the model with a greater number of review results to confirm this accuracy.  As an example, the spread of the allocation of scores from A to E should add some effectively random noise (effectively human error due to over 9,000 judgements required) that the model would be unable to correctly predict.



