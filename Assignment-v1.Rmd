---
title: "Practical Machine Learning Assignment "
output: html_document
---


## Practical Machine Learning Assignment

###Executive summary
Three models were created to predict the quality of the performance of a prescribed exercise, based on movement sensor data provided.  Two models were effectively unused as the initial random forest model provided as expected forecast success rate of virtually 100%.  When the forecasts were compared against the actual effectiveness of the technique, the model created an astonishing 100% success rate using 52 predictor variables.  Based on this, the model can be considered an excellent method to predict given these predictors.  Further work could further identify the most critical of those predictor variables.  

###Background
Much of the literature surrounding exercise focusses on the quantity of exercise performed, with less attention paid to the ‘quality’ of the exercise performed.  In this study, the movements of six participants, who had limited previous experience lifting weights, were captured using accelerometers as they performed barbell lifts with light weights, and a score (from A to E) was given for the 'correctness' of the performance of the exercise.  This base data was then analysed, and using machine learning functions a model to assess the technical accuracy (ie how well) of each individuals exercise.  The resulting model was then used to predict whether an exercise was correctly performed on an semi-independent testing dataset.  
The data is publically available under the Creative Commons license (CC BY-SA) licence at http://groupware.les.inf.puc-rio.br/har

###Exploratory data analysis
First, the original data must be loaded into R
```{r, message = FALSE}
mydata<-read.csv("c:/pml_assignment/ass_train.csv")
```
Then the data table was reviewed by eye to try to identify any superfluous data columns and a simple data cleaning process undertaken to remove columns with no data, significant proportion of NA’s, or irrelevant data (eg names and timestamps).  The alphabetic results for the accuracy of the exercise (the A to E scoring) was also converted to a form more readily usable by R.
```{r}
mydata$classe <- factor(mydata$classe)
mydata<-subset(mydata, select = c(8:11, 37:49, 60:68,84:86, 102, 113:124, 140, 151:160))
```


The data must be partitioned into a training (from which the initial algorithm is built) and testing (a dataset used to test the models) subsets.  The standard 60:40 split has been utilised.
```{r, message = FALSE}
library(caret)
set.seed(676767)   # for repeatability
srcData <- createDataPartition(y = mydata$classe, p=0.6,list=FALSE);
train <- mydata[srcData,];
test <- mydata[-srcData,];
```


###Model selection
Then the actual models are created.  In order to try and ensure the highest possible accuracy in the forecast, three models will be built and assessed for accuracy against the test subset of data.  The functions used to create the models will use all 52 available variables.  The most accurate model will then be used to predict the exercise quality in the review data.   Three models will be created using the random forest,  boosted trees and linear discriminant algorithms.  
```{r, message = FALSE}
library(randomForest)
rf <- randomForest( classe ~., data=train, ntrees=20)
library(MASS)
lda1<- lda(classe ~. , train)
library(gbm)
gbm_model<- gbm(classe ~. ,data= mydata, distribution = "gaussian" ,n.trees = 30)
```

###Initial review
Initially, the predictive power of the three models must be assessed to identify the best, which would then be used going forward.  Basically, here the best model for this dataset (test) is chosen.  The models are assessed against the original test dataset - basically how well they would predict the data used to create them.  
For the random forest (rf) model, the result is: 
```{r}
rf_train <- predict(rf, train)
print(confusionMatrix(rf_train, train$classe))

```
Basically, this shows the RF model to be 100% accurate, with a p-value of effectively 0.  Generally a result of this nature would cause concern over the model, and would render much statistical analysis unreliable (eg ANOVA).  HOwever, this is a simple exercise, so we will accept the models results as feasible, and not an error.

Given the extremely high predictive power of the random forest model, review of the other models appears futile and so will not be undertaken.  Basically, they are extremely unlikely to be as good.

###Cross validation
The  model chosen will be the best for the test dastaset, as it was chosen for exactly that reason.  To try and minimise, identify and possibly remove any inherent errors or overfitting, a cross validation process was undertaken.  This will also give a reasonable indication as to how the model will evaluate the review (independent) dataset.
```{r}
rf_test <- predict(rf, test)
print(confusionMatrix(rf_test, test$classe))
```

The cross validation process shows the model remains extremely accurate when prediciting against the test dataset, with accuracy of 99.36% and p-value remaining at effectively 0.  Therefore, it appears reasonable to apply against the model to the independent dataset.  

###Prediction
Now the model is applied to the independent data to review its accuracy.  This is the final step and can only be applied once.  If done more than this the independent data would become part of the intial dataset as the model will be influenced to fit this data.
First the independent data file is read into R, then the model is applied to this, and the resultant predictions shown.
```{r}
rev<-read.csv("c:/mba/ass_review.csv")
rf_predict <- predict(rf,rev)
rf_predict
```

The results are then transferred to a vector array, a separate text file is created for each predicted value and these were assessed to the actual results.  

```{r, eval=FALSE}
setwd("c:/pml_assignment/")                   # set working dir
answers <- as.vector(rf_review)    # create vector list
pml_write_files(answers)          
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
```
The review showed the model to be 100% accurate, an astonishing hit rate.  It would be interesting to assess with a greater number of review results to confirm this accuracy.  As an example, the spread of the allocation of scores from A to E should add some effectively random jitter (effectively human error with over 9,000 observations) that the model would be unable to compensate for.



